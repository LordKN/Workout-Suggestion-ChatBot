import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, r2_score, accuracy_score, classification_report
from sklearn.impute import SimpleImputer

# Set random seed for reproducibility
np.random.seed(42)

def load_and_explore_data():
    """Load and explore the dataset."""
    print("Loading data...")
    df = pd.read_csv('members_with_exercise_recommendations.csv')
    
    print(f"Dataset shape: {df.shape}")
    print("\nFirst few rows:")
    print(df.head())
    
    print("\nBasic statistics:")
    print(df.describe())
    
    print("\nMissing values:")
    print(df.isnull().sum())
    
    return df

def preprocess_data(df):
    """Preprocess the data for machine learning."""
    print("\nPreprocessing data...")
    
    # Drop columns that are not useful for prediction
    # We'll drop the recommended exercises and details as they were generated by our script
    df_ml = df.drop(['Recommended_Exercises', 'Exercise_Type_Details'], axis=1)
    
    # Check for missing values and handle them
    if df_ml.isnull().sum().sum() > 0:
        print("Handling missing values...")
        # For simplicity, we'll use median imputation for numeric columns
        numeric_cols = df_ml.select_dtypes(include=['int64', 'float64']).columns
        df_ml[numeric_cols] = df_ml[numeric_cols].fillna(df_ml[numeric_cols].median())
        
        # For categorical columns, fill with the most frequent value
        cat_cols = df_ml.select_dtypes(include=['object']).columns
        for col in cat_cols:
            df_ml[col] = df_ml[col].fillna(df_ml[col].mode()[0])
    
    return df_ml

def calories_burned_prediction(df):
    """Build a model to predict calories burned."""
    print("\n--- Calories Burned Prediction Model ---")
    
    # Define features and target
    X = df.drop(['Calories_Burned'], axis=1)
    y = df['Calories_Burned']
    
    # Identify numeric and categorical columns
    numeric_cols = X.select_dtypes(include=['int64', 'float64']).columns
    categorical_cols = X.select_dtypes(include=['object']).columns
    
    # Create preprocessing steps for numeric and categorical data
    numeric_transformer = Pipeline(steps=[
        ('imputer', SimpleImputer(strategy='median')),
        ('scaler', StandardScaler())
    ])
    
    categorical_transformer = Pipeline(steps=[
        ('imputer', SimpleImputer(strategy='most_frequent')),
        ('onehot', OneHotEncoder(handle_unknown='ignore'))
    ])
    
    # Combine preprocessing steps
    preprocessor = ColumnTransformer(
        transformers=[
            ('num', numeric_transformer, numeric_cols),
            ('cat', categorical_transformer, categorical_cols)
        ])
    
    # Create and evaluate models
    models = {
        'Linear Regression': LinearRegression(),
        'Random Forest': RandomForestRegressor(n_estimators=100, random_state=42)
    }
    
    # Split data
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
    
    for name, model in models.items():
        # Create pipeline with preprocessing and model
        pipeline = Pipeline(steps=[
            ('preprocessor', preprocessor),
            ('model', model)
        ])
        
        # Train model
        print(f"\nTraining {name}...")
        pipeline.fit(X_train, y_train)
        
        # Make predictions
        y_pred = pipeline.predict(X_test)
        
        # Evaluate model
        mse = mean_squared_error(y_test, y_pred)
        r2 = r2_score(y_test, y_pred)
        
        print(f"{name} Results:")
        print(f"Mean Squared Error: {mse:.2f}")
        print(f"RÂ² Score: {r2:.2f}")
        print(f"RMSE: {np.sqrt(mse):.2f}")
        
        # For Random Forest, show feature importance
        if name == 'Random Forest':
            # Get feature names after one-hot encoding
            feature_names = []
            for name, transformer, cols in preprocessor.transformers_:
                if name == 'cat':
                    # Get the one-hot encoded feature names
                    feature_names.extend(transformer.named_steps['onehot'].get_feature_names_out(cols))
                else:
                    feature_names.extend(cols)
            
            # Get feature importances
            importances = pipeline.named_steps['model'].feature_importances_
            
            # Sort feature importances in descending order
            indices = np.argsort(importances)[::-1]
            
            # Print the feature ranking
            print("\nFeature ranking:")
            for f in range(min(10, len(feature_names))):
                if f < len(indices):
                    print(f"{f+1}. {feature_names[indices[f]]} ({importances[indices[f]]:.4f})")
    
    return pipeline  # Return the last trained model

def workout_type_prediction(df):
    """Build a model to predict workout type."""
    print("\n--- Workout Type Prediction Model ---")
    
    # Define features and target
    X = df.drop(['Workout_Type', 'Calories_Burned'], axis=1)  # We'll exclude calories as it's highly correlated with workout type
    y = df['Workout_Type']
    
    # Identify numeric and categorical columns
    numeric_cols = X.select_dtypes(include=['int64', 'float64']).columns
    categorical_cols = X.select_dtypes(include=['object']).columns
    
    # Create preprocessing steps
    numeric_transformer = Pipeline(steps=[
        ('imputer', SimpleImputer(strategy='median')),
        ('scaler', StandardScaler())
    ])
    
    categorical_transformer = Pipeline(steps=[
        ('imputer', SimpleImputer(strategy='most_frequent')),
        ('onehot', OneHotEncoder(handle_unknown='ignore'))
    ])
    
    # Combine preprocessing steps
    preprocessor = ColumnTransformer(
        transformers=[
            ('num', numeric_transformer, numeric_cols),
            ('cat', categorical_transformer, categorical_cols)
        ])
    
    # Create model
    model = RandomForestClassifier(n_estimators=100, random_state=42)
    
    # Create pipeline
    pipeline = Pipeline(steps=[
        ('preprocessor', preprocessor),
        ('model', model)
    ])
    
    # Split data
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
    
    # Train model
    print("Training Random Forest Classifier...")
    pipeline.fit(X_train, y_train)
    
    # Make predictions
    y_pred = pipeline.predict(X_test)
    
    # Evaluate model
    accuracy = accuracy_score(y_test, y_pred)
    
    print(f"Accuracy: {accuracy:.2f}")
    print("\nClassification Report:")
    print(classification_report(y_test, y_pred))
    
    return pipeline

def experience_level_prediction(df):
    """Build a model to predict experience level."""
    print("\n--- Experience Level Prediction Model ---")
    
    # Define features and target
    X = df.drop(['Experience_Level', 'Workout_Type'], axis=1)  # Exclude workout type as it might leak information
    y = df['Experience_Level']
    
    # Identify numeric and categorical columns
    numeric_cols = X.select_dtypes(include=['int64', 'float64']).columns
    categorical_cols = X.select_dtypes(include=['object']).columns
    
    # Create preprocessing steps
    numeric_transformer = Pipeline(steps=[
        ('imputer', SimpleImputer(strategy='median')),
        ('scaler', StandardScaler())
    ])
    
    categorical_transformer = Pipeline(steps=[
        ('imputer', SimpleImputer(strategy='most_frequent')),
        ('onehot', OneHotEncoder(handle_unknown='ignore'))
    ])
    
    # Combine preprocessing steps
    preprocessor = ColumnTransformer(
        transformers=[
            ('num', numeric_transformer, numeric_cols),
            ('cat', categorical_transformer, categorical_cols)
        ])
    
    # Create model
    model = RandomForestClassifier(n_estimators=100, random_state=42)
    
    # Create pipeline
    pipeline = Pipeline(steps=[
        ('preprocessor', preprocessor),
        ('model', model)
    ])
    
    # Split data
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
    
    # Train model
    print("Training Random Forest Classifier...")
    pipeline.fit(X_train, y_train)
    
    # Make predictions
    y_pred = pipeline.predict(X_test)
    
    # Evaluate model
    accuracy = accuracy_score(y_test, y_pred)
    
    print(f"Accuracy: {accuracy:.2f}")
    print("\nClassification Report:")
    print(classification_report(y_test, y_pred))
    
    return pipeline

def visualize_data(df):
    """Create visualizations of the data."""
    print("\nCreating visualizations...")
    
    # Create a directory for visualizations
    import os
    if not os.path.exists('visualizations'):
        os.makedirs('visualizations')
    
    # 1. Distribution of workout types
    plt.figure(figsize=(10, 6))
    workout_counts = df['Workout_Type'].value_counts()
    sns.barplot(x=workout_counts.index, y=workout_counts.values)
    plt.title('Distribution of Workout Types')
    plt.xlabel('Workout Type')
    plt.ylabel('Count')
    plt.xticks(rotation=45)
    plt.tight_layout()
    plt.savefig('visualizations/workout_types.png')
    
    # 2. Calories burned by workout type
    plt.figure(figsize=(10, 6))
    sns.boxplot(x='Workout_Type', y='Calories_Burned', data=df)
    plt.title('Calories Burned by Workout Type')
    plt.xlabel('Workout Type')
    plt.ylabel('Calories Burned')
    plt.xticks(rotation=45)
    plt.tight_layout()
    plt.savefig('visualizations/calories_by_workout.png')
    
    # 3. Correlation heatmap
    plt.figure(figsize=(12, 10))
    numeric_df = df.select_dtypes(include=['int64', 'float64'])
    correlation = numeric_df.corr()
    mask = np.triu(np.ones_like(correlation, dtype=bool))
    sns.heatmap(correlation, mask=mask, annot=True, fmt='.2f', cmap='coolwarm', square=True, linewidths=.5)
    plt.title('Correlation Heatmap')
    plt.tight_layout()
    plt.savefig('visualizations/correlation_heatmap.png')
    
    # 4. Age distribution by gender
    plt.figure(figsize=(10, 6))
    sns.histplot(data=df, x='Age', hue='Gender', multiple='stack', bins=20)
    plt.title('Age Distribution by Gender')
    plt.xlabel('Age')
    plt.ylabel('Count')
    plt.tight_layout()
    plt.savefig('visualizations/age_by_gender.png')
    
    # 5. BMI distribution
    plt.figure(figsize=(10, 6))
    sns.histplot(data=df, x='BMI', bins=20)
    plt.title('BMI Distribution')
    plt.xlabel('BMI')
    plt.ylabel('Count')
    plt.tight_layout()
    plt.savefig('visualizations/bmi_distribution.png')
    
    print("Visualizations saved to 'visualizations' directory.")

def make_predictions(calories_model, workout_model, experience_model):
    """Make predictions for a new gym member."""
    print("\n--- Making Predictions for a New Gym Member ---")

    # Create a sample new member
    new_member = pd.DataFrame({
        'Age': [30],
        'Gender': ['Male'],
        'Weight (kg)': [75.0],
        'Height (m)': [1.75],
        'Max_BPM': [180],
        'Avg_BPM': [150],
        'Resting_BPM': [65],
        'Session_Duration (hours)': [1.0],
        'Fat_Percentage': [20.0],
        'Water_Intake (liters)': [2.5],
        'Workout_Frequency (days/week)': [3],
        'BMI': [24.5],
        # Dummy values for required columns
        'Workout_Type': ['Strength'],  
        'Experience_Level': [2],       
        'Calories_Burned': [0]         
    })

    # Prepare input for the calories prediction model
    new_member_calories = new_member.drop(['Calories_Burned'], axis=1)

    # Predict calories burned
    predicted_calories = calories_model.predict(new_member_calories)

    # Ensure it is a single scalar value
    predicted_calories = float(predicted_calories[0])  # Convert first element to a float

    print("\nPredictions for new gym member:")
    print(f"Predicted calories burned: {predicted_calories:.2f}")

    return predicted_calories  # Return a scalar float instead of an array


def generate_meal_recommendations(predicted_calories, nutrient_file, used_foods=None):
    """Generate meal recommendations based on calories burned, ensuring variety across days."""
    print("\n--- Generating Meal Recommendations ---")
    
    # Load nutrient data
    nutrients_df = pd.read_csv(nutrient_file)

    # Ensure required columns exist
    required_columns = {'Food', 'Calories', 'Protein', 'Carbs', 'Fiber'}
    if not required_columns.issubset(nutrients_df.columns):
        raise ValueError(f"Missing required columns in nutrient file: {required_columns - set(nutrients_df.columns)}")

    # Convert Calories, Protein, Carbs, and Fiber to numeric
    nutrients_df['Calories'] = pd.to_numeric(nutrients_df['Calories'], errors='coerce')
    nutrients_df['Protein'] = pd.to_numeric(nutrients_df['Protein'], errors='coerce')
    nutrients_df['Carbs'] = pd.to_numeric(nutrients_df['Carbs'], errors='coerce')
    nutrients_df['Fiber'] = pd.to_numeric(nutrients_df['Fiber'], errors='coerce')

    # Drop any rows with NaN values after conversion
    nutrients_df = nutrients_df.dropna()

    # Shuffle food options to ensure variety
    nutrients_df = nutrients_df.sample(frac=1).reset_index(drop=True)

    if used_foods is None:
        used_foods = set()

    meal_plan = {}
    remaining_calories = predicted_calories

    print(f"Total Calories Target: {predicted_calories:.2f}")

    for _, food in nutrients_df.iterrows():
        if remaining_calories <= 0:
            break
        
        if food['Food'] in used_foods:
            continue  # Skip foods already recommended in previous days
        
        used_foods.add(food['Food'])  # Mark food as used
        
        if food['Food'] in meal_plan:
            meal_plan[food['Food']]['Servings'] += 1
        else:
            meal_plan[food['Food']] = {
                'Servings': 1,
                'Calories': food['Calories'],
                'Protein': food['Protein'],
                'Carbs': food['Carbs'],
                'Fiber': food['Fiber']
            }

        remaining_calories -= food['Calories']

    print("Recommended Foods:")
    for food, details in meal_plan.items():
        total_calories = details['Calories'] * details['Servings']
        total_protein = details['Protein'] * details['Servings']
        total_carbs = details['Carbs'] * details['Servings']
        total_fiber = details['Fiber'] * details['Servings']

        print(f"- {food} (x{details['Servings']}): {total_calories:.1f} calories, {total_protein:.1f}g protein, {total_carbs:.1f}g carbs, {total_fiber:.1f}g fiber")

    return meal_plan



def main():
    """Main function to run the machine learning pipeline."""
    print("=== Gym Member Machine Learning Models ===")

    # Load and preprocess data
    df = load_and_explore_data()
    df_ml = preprocess_data(df)

    # Visualize data
    visualize_data(df)

    # Train models
    calories_model = calories_burned_prediction(df_ml)
    workout_model = workout_type_prediction(df_ml)
    experience_model = experience_level_prediction(df_ml)

    # Get the predicted calories burned
    predicted_calories = make_predictions(calories_model, workout_model, experience_model)

    # Generate meal recommendations based on predicted calories
    nutrient_file = "nutrients_csvfile.csv"
    generate_meal_recommendations(predicted_calories, nutrient_file)

    print("\nDone!")




if __name__ == "__main__":
    main() 
    